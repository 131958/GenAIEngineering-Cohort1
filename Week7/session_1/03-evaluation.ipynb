{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3: Multimodal Evaluation\n",
    "\n",
    "[Click to view on Google Colab](https://colab.research.google.com/drive/1FR9Ua8VoAPI-nYlgXagiRhOSGq40cDGC?usp=sharing)\n",
    "  \n",
    "### Metrics Used:\n",
    "- CLIP: Score ranges from 0-100 (higher = better)\n",
    "- BLEU: Score ranges from 0-1 (higher = better)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import evaluate\n",
    "\n",
    "# Fix tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_fast.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing\n",
    "\n",
    "Converts PIL images to tensors compatible with CLIP scoring. Standardizes image format to 224x224 pixels and proper tensor format for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"Convert PIL image to tensor for CLIP Score\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: (x * 255).byte())\n",
    "    ])\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Generation\n",
    "\n",
    "Creates synthetic image-caption pairs for demonstration:\n",
    "- Solid colored squares (red, blue, green)\n",
    "- Simple geometric patterns (circles, stripes)\n",
    "- Utility functions for random sampling and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_pattern(color_rgb, pattern_type):\n",
    "    \"\"\"Create simple patterned images\"\"\"\n",
    "    img = Image.new('RGB', (224, 224), 'white')\n",
    "    pixels = img.load()\n",
    "    \n",
    "    if pattern_type == 'circle':\n",
    "        # Draw a simple circle\n",
    "        center_x, center_y = 112, 112\n",
    "        radius = 80\n",
    "        for x in range(224):\n",
    "            for y in range(224):\n",
    "                if (x - center_x)**2 + (y - center_y)**2 <= radius**2:\n",
    "                    pixels[x, y] = color_rgb\n",
    "    \n",
    "    elif pattern_type == 'stripes':\n",
    "        # Draw horizontal stripes\n",
    "        for x in range(224):\n",
    "            for y in range(224):\n",
    "                if (y // 20) % 2 == 0:  # Every 20 pixels\n",
    "                    pixels[x, y] = color_rgb\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def create_sample_examples():\n",
    "    \"\"\"Create simple sample examples for demo\"\"\"\n",
    "    examples = [\n",
    "        {\n",
    "            \"image\": Image.new('RGB', (224, 224), 'red'),\n",
    "            \"caption\": \"A red colored square\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": Image.new('RGB', (224, 224), 'blue'),\n",
    "            \"caption\": \"A blue colored square\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": Image.new('RGB', (224, 224), 'green'),\n",
    "            \"caption\": \"A green colored square\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": create_simple_pattern((255, 255, 0), 'circle'),\n",
    "            \"caption\": \"A yellow colored circle on white background\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": create_simple_pattern((128, 0, 128), 'stripes'),\n",
    "            \"caption\": \"Purple and white colored stripes\"\n",
    "        }\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "def get_random_example():\n",
    "    \"\"\"Get a random image-caption pair\"\"\"\n",
    "    examples = create_sample_examples()\n",
    "    example = random.choice(examples)\n",
    "    print(\"âœ… Generated synthetic example\")\n",
    "    return example[\"image\"], example[\"caption\"]\n",
    "\n",
    "def display_image(image, caption=None):\n",
    "    \"\"\"Display image with optional caption\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    if caption:\n",
    "        plt.title(f\"Caption: '{caption}'\", fontsize=12, pad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Core evaluation functions implementing:\n",
    "- **CLIP Score**: Measures semantic similarity between images and text\n",
    "- **Caption Comparison**: Evaluates multiple captions against single image\n",
    "- **Demo Functions**: Interactive examples showing evaluation in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_caption(image, caption):\n",
    "    \"\"\"Test a caption against an image and return score\"\"\"\n",
    "    image_tensor = preprocess_image(image)\n",
    "    score = clip_score(image_tensor, caption, \"openai/clip-vit-base-patch16\")\n",
    "    return score.item()\n",
    "\n",
    "def compare_captions(image, captions):\n",
    "    \"\"\"Compare multiple captions for the same image\"\"\"\n",
    "    print(\"ðŸ“Š Comparing captions:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    scores = []\n",
    "    for i, caption in enumerate(captions, 1):\n",
    "        score = evaluate_caption(image, caption)\n",
    "        print(f\"{i}. '{caption}' â†’ {score:.1f}\")\n",
    "        scores.append(score)\n",
    "    \n",
    "    print(f\"\\nBest score: {max(scores):.1f}\")\n",
    "    print(f\"Worst score: {min(scores):.1f}\")\n",
    "    return scores\n",
    "\n",
    "# Example usage functions\n",
    "def demo_clip_basic():\n",
    "    \"\"\"Basic demo - load image and test original caption\"\"\"\n",
    "    print(\"ðŸš€ Basic CLIP Score Demo\")\n",
    "    image, caption = get_random_example()\n",
    "    display_image(image, caption)\n",
    "    score = evaluate_caption(image, caption)\n",
    "    print(f\"Original caption score: {score:.1f}\")\n",
    "    return image, caption\n",
    "\n",
    "def demo_clip_comparison():\n",
    "    \"\"\"Demo comparing multiple captions\"\"\"\n",
    "    print(\"ðŸš€ Caption Comparison Demo\")\n",
    "    image, original_caption = get_random_example()\n",
    "    display_image(image, original_caption)\n",
    "    \n",
    "    # Test multiple captions\n",
    "    test_captions = [\n",
    "        original_caption,\n",
    "        \"A car driving on a road\",\n",
    "        \"A human playing a guitar\",\n",
    "        \"A colorful image\"\n",
    "    ]\n",
    "    \n",
    "    scores = compare_captions(image, test_captions)\n",
    "    return image, test_captions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_clip_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_clip_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score Demonstration\n",
    "\n",
    "Comparison between BLEU and CLIP metrics:\n",
    "- **Exact Match Requirement**: BLEU needs precise word overlap\n",
    "- **Semantic Understanding**: CLIP captures meaning beyond exact words\n",
    "- **Practical Implications**: Shows why CLIP is better for multimodal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_bleu_simple():\n",
    "    \"\"\"Simple BLEU score demonstration with clear examples\"\"\"\n",
    "    print(\"ðŸš€ Simple BLEU Demo\")\n",
    "    print(\"BLEU measures exact word overlap (0-1 scale)\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    # Get example\n",
    "    image, reference = get_random_example()\n",
    "    display_image(image, reference)\n",
    "    \n",
    "    print(f\"Reference: '{reference}'\")\n",
    "    print(\"\\nTesting 3 simple cases:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Simple test cases\n",
    "    test_cases = [\n",
    "        (reference, \"Identical text\"),\n",
    "        (reference.replace(\"colored\", \"color\"), \"One word changed\"),\n",
    "        (\"Something completely different\", \"Totally different\")\n",
    "    ]\n",
    "    \n",
    "    for candidate, description in test_cases:\n",
    "        # BLEU score\n",
    "        bleu_result = bleu_metric.compute(\n",
    "            predictions=[candidate], \n",
    "            references=[[reference]]\n",
    "        )\n",
    "        bleu_score = bleu_result['bleu']\n",
    "        \n",
    "        # CLIP score\n",
    "        clip_score_val = evaluate_caption(image, candidate)\n",
    "        \n",
    "        print(f\"Test: {description}\")\n",
    "        print(f\"Text: '{candidate}'\")\n",
    "        print(f\"BLEU: {bleu_score:.3f} | CLIP: {clip_score_val:.1f}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"ðŸ’¡ Key Point:\")\n",
    "    print(\"BLEU needs exact word matches. Even small changes â†’ BLEU = 0\")\n",
    "    print(\"CLIP understands meaning, so similar concepts get higher scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_bleu_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs_w7_s1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
