{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevant Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "import csv\n",
    "from groq import Groq\n",
    "import dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQLite DB**  \n",
    "Let us connect to the SQLite Sample Database what we have  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an engine instance created, which can handle multiple connetions\n",
    "sql_engine = create_engine(\"sqlite:///Sample_2 - Copy.db\")\n",
    "conn_1 = sql_engine.connect ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Query**  \n",
    "Given a user prompt and table schema, LLM can generate SQL query  \n",
    "The query generation requires specific instructions, so that it can be used correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv ()\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Instr = \"From the given SQL table schema, formulate SQL query which answers user's question.\\\n",
    "        Respond only the SQL query. No title, no introduction.\"\n",
    "\n",
    "Schema = \"\"\"\n",
    "            CREATE TABLE Student_Performance (\n",
    "            student_id VARCHAR(50),\n",
    "            age INTEGER,\n",
    "            gender VARCHAR(50),\n",
    "            study_hours_per_day REAL,\n",
    "            social_media_hours REAL,\n",
    "            netflix_hours REAL,\n",
    "            part_time_job VARCHAR(50),\n",
    "            attendance_percentage REAL,\n",
    "            sleep_hours REAL,\n",
    "            diet_quality VARCHAR(50),\n",
    "            exercise_frequency INTEGER,\n",
    "            parental_education_level VARCHAR(50),\n",
    "            internet_quality VARCHAR(50),\n",
    "            mental_health_rating INTEGER,\n",
    "            extracurricular_participation VARCHAR(50),\n",
    "            exam_score REAL\n",
    "          )\n",
    "        \"\"\"\n",
    "\n",
    "Prompt = \"On an average students watch netflix for ...\"\n",
    "# Prompt = \"Are students taking enough rest?\"\n",
    "# Prompt = \"Is there a correlation between study time and score really?\"\n",
    "# Prompt = \"Tell me top 5 scorer who are doing a job additionally, because its quite challenging\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": Q_Instr\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Schema :\\n\"+Schema+\"\\n Question : \\n\"+Prompt\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"llama3-70b-8192\",\n",
    "\n",
    ")\n",
    "\n",
    "print (completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Additional instructions to fine tune the query generation  \n",
    ">This will help to get precise output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Instr = \"From the given SQL table schema, formulate SQL query which answers user's question.\\\n",
    "        Respond only the SQL query. No title, no introduction. Give complete query.\\\n",
    "        Its SQLite DB.\\\n",
    "        When searching text field, always bring to lower case and use wild card : %xxx% .\\\n",
    "        If question is not relevant to schema, say 'No relevant data'\"\n",
    "\n",
    "\n",
    "Schema = \"\"\"\n",
    "            CREATE TABLE Student_Performance (\n",
    "            student_id VARCHAR(50),\n",
    "            age INTEGER,\n",
    "            gender VARCHAR(50),\n",
    "            study_hours_per_day REAL,\n",
    "            social_media_hours REAL,\n",
    "            netflix_hours REAL,\n",
    "            part_time_job VARCHAR(50),\n",
    "            attendance_percentage REAL,\n",
    "            sleep_hours REAL,\n",
    "            diet_quality VARCHAR(50),\n",
    "            exercise_frequency INTEGER,\n",
    "            parental_education_level VARCHAR(50),\n",
    "            internet_quality VARCHAR(50),\n",
    "            mental_health_rating INTEGER,\n",
    "            extracurricular_participation VARCHAR(50),\n",
    "            exam_score REAL\n",
    "          )\n",
    "        \"\"\"\n",
    "\n",
    "Prompt = \"Is there a correlation between study time and score really?\"\n",
    "# Prompt = \"Tell me top 5 scorers who are doing a job additionally, because its quite challenging\"\n",
    "# Prompt = \"What are the kind of diet habits they have?\"\n",
    "# Prompt = \"When are the exams?\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": Q_Instr\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Schema :\\n\"+Schema+\"\\n Question : \\n\"+Prompt\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"llama3-70b-8192\",   \n",
    "\n",
    ")\n",
    "\n",
    "Query_String = completion.choices[0].message.content\n",
    "\n",
    "print (Query_String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete Pipeline**\n",
    "The query string further needs to be used for extraction, provide the context from there and get the answer from LLM  \n",
    "First invoke of LLM to get the SQL Query. Then next one for getting the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = \"how many students are studying more?\"\n",
    "# Prompt = \"Who are the top 5 scorers and how is their social media, netflix habits?\"\n",
    "# Prompt = \"Considering the top 10 students, do you think social media time and netflix matters?\"\n",
    "# Prompt = \"students whose parents have completed High school, have scored more than 90 in exams\"\n",
    "\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": Q_Instr\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Schema :\\n\"+Schema+\"\\n Question : \\n\"+Prompt\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"llama3-70b-8192\",   \n",
    "\n",
    ")\n",
    "\n",
    "## Get the query string\n",
    "Query_String = completion.choices[0].message.content\n",
    "print(Query_String)\n",
    "\n",
    "#Fetch the data from DB\n",
    "result = conn_1.execute (text(Query_String))\n",
    "\n",
    "# Query output into JSON\n",
    "rows = [row._asdict () for row in result]\n",
    "json_output = json.dumps(rows,  indent=2)\n",
    "\n",
    "R_Instr = \"Using the context given, provide response to the user question or statement.\\\n",
    "            Context is provided as JSON information.\\\n",
    "            Answer to the question precisely\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": R_Instr\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\"Context : \\n\"+ json_output + \"Query : \\n\" + Prompt\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "print (completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Str",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
