{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-read for RAG Module Sessions 1 & 2: Building the Retrieval Foundation**\n",
    "\n",
    "This document provides a high-level overview of the first two sessions of the Retrieval-Augmented Generation (RAG) module, covering necessary installations, topics, and key learning objectives. We will focus on building the essential data preparation and retrieval components of a RAG system, with hands-on work centered around using the **Qdrant vector database** in its **local, in-memory configuration** via the `qdrant-client` library.\n",
    "\n",
    "### **Installation Prerequisites**\n",
    "\n",
    "To follow along with the practical exercises, you will need Python (3.8+ recommended) and the following libraries installed. We highly recommend using a virtual environment.\n",
    "\n",
    "Run the following command in your terminal:\n",
    "\n",
    "```shell\n",
    "    pip install langchain-text-splitters langchain-community langchain-huggingface langchain-qdrant sentence-transformers qdrant-client numpy scikit-learn rank_bm25\n",
    "```\n",
    "\n",
    "This command installs libraries for:\n",
    "\n",
    "* Text splitting (`langchain-text-splitters`).  \n",
    "* LangChain integrations for embeddings and vector stores (`langchain-community`, `langchain-huggingface`, `langchain-qdrant`).  \n",
    "* Generating vector embeddings (`sentence-transformers`).  \n",
    "* Interacting with Qdrant locally (`qdrant-client`).  \n",
    "* Numerical operations (`numpy`).  \n",
    "* Keyword search concepts (`scikit-learn` for TF-IDF, `rank_bm25` for BM25).\n",
    "\n",
    "**Note on Qdrant:** For these sessions, we will use the `qdrant-client` library to run Qdrant entirely in your local process memory (`location=\":memory:\"`). This requires **no separate Qdrant server installation or Docker setup**.\n",
    "\n",
    "### **Session 1: Foundations \\- Introduction to RAG and Document Preparation**\n",
    "\n",
    "**Session Goal:** Introduce the core concepts of RAG, explain why it's necessary, outline its workflow, and delve into preparing your data by chunking documents. We will also introduce the basics of keyword search and the fundamental idea of vectorization for semantic understanding.\n",
    "\n",
    "**Topics You Will Learn:**\n",
    "\n",
    "* **What is RAG and Why Use It?** Understand RAG as an architectural pattern to enhance LLMs by giving them access to external knowledge. Learn how RAG helps overcome LLM limitations like static knowledge, hallucination, and lack of access to private data. Explore various use cases.  \n",
    "* **Core RAG Architecture & Workflow:** Identify the main components: External Knowledge Source (Vector Database), Retriever, and Generator (LLM). Understand the Ingestion (data preparation, indexing) and Inference (query processing, retrieval, generation) stages.  \n",
    "* **Document Preparation: Chunking:** Learn why large documents must be split into smaller \"chunks\" due to LLM context window limitations. Understand that chunking strategy impacts retrieval quality.  \n",
    "* **Chunking Strategies:** Cover practical methods using LangChain:  \n",
    "  * **Fixed-Size Chunking:** Splitting by a fixed character count, with optional overlap.  \n",
    "  * **Recursive Character Text Splitting:** Splitting using a list of separators to preserve semantic units.  \n",
    "  * *(Other strategies like Semantic or Document-Based chunking will be mentioned conceptually)*.  \n",
    "* **Keyword Search Fundamentals:** Basic concepts like TF-IDF and BM25 will be introduced as traditional methods relying on exact word matching. Contrast this with semantic understanding.  \n",
    "* **Introduction to Vectorization:** Learn how text is converted into numerical vectors (embeddings) that capture meaning. Understand that texts with similar meanings are close in vector space. Introduction to **Transformer models** and **Sentence Transformers** like `all-MiniLM-L6-v2` for creating these embeddings.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "After Session 1, you should be able to:\n",
    "\n",
    "* Explain the purpose and workflow of a RAG system.  \n",
    "* Understand the need for and challenges of document chunking.  \n",
    "* **Implement fixed-size and recursive text splitting using LangChain**.  \n",
    "* Explain the difference between keyword search and semantic search.  \n",
    "* Understand the concept of vector embeddings and how they represent text meaning numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Prereqs\n",
    "!pip install -qq langchain-text-splitters langchain-community langchain-huggingface langchain-qdrant sentence-transformers qdrant-client numpy scikit-learn rank_bm25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-req steps to use the Gemini API key with Python:\n",
    "\n",
    "1.  **Get API Key:** Go to [aistudio.google.com](https://aistudio.google.com/), sign in, and create/copy your API key (secure it immediately).\n",
    "2.  **Install Library:** In your terminal, run `pip install -q -U google-generativeai`.\n",
    "3.  **Set API Key Securely:**\n",
    "    * **Recommended:** Set it as an environment variable (e.g., `export GOOGLE_API_KEY=\"YOUR_KEY\"` in terminal/shell config).\n",
    "    * **Colab:** Use Colab Secrets to store `GOOGLE_API_KEY`.\n",
    "4.  **Configure in Python:** In your script, retrieve the key (e.g., `os.getenv('GOOGLE_API_KEY')` or `userdata.get('GOOGLE_API_KEY')`) and then use `genai.configure(api_key=YOUR_KEY)`.\n",
    "\n",
    "[Gemini API Key](https://aistudio.google.com/u/1/apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/GenAIEngineering-Cohort1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv  # For loading API key from a .env file\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "model_name = 'gemini-2.0-flash'\n",
    "\n",
    "model = genai.GenerativeModel(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is RAG?**\n",
    "\n",
    "RAG is an AI framework that enhances the capabilities of LLMs by enabling them to access, retrieve, and incorporate external, up-to-date, and authoritative information into their responses. It combines the strengths of two distinct paradigms:\n",
    "\n",
    "1. **Retrieval:** The ability to efficiently search and retrieve relevant information from a vast corpus of documents.  \n",
    "2. **Generation:** The LLM's ability to generate coherent and contextually relevant text.\n",
    "\n",
    "In essence, RAG acts as a bridge, allowing LLMs to look up information in a knowledge base *before* formulating a response, much like a human would consult a textbook or database to answer a complex question.\n",
    "\n",
    "### **Why is RAG Needed?**\n",
    "\n",
    "RAG addresses several critical limitations of standalone LLMs:\n",
    "\n",
    "1. **Mitigating Hallucinations:** One of the most significant challenges with LLMs is their tendency to invent information, especially when they don't have a confident answer. By grounding their responses in retrieved facts, RAG significantly reduces the likelihood of hallucinations, leading to more reliable and trustworthy outputs.  \n",
    "2. **Access to Up-to-Date Information:** LLMs have a knowledge cutoff date, meaning they cannot access information beyond their last training update. RAG allows them to query continually updated databases, ensuring their responses are current and accurate, vital for dynamic fields like news, finance, or scientific research.  \n",
    "3. **Domain-Specific Knowledge:** Pre-trained LLMs are generalists. They lack specific knowledge about a particular company's internal documents, proprietary data, or niche industry information. RAG enables LLMs to tap into these private, domain-specific knowledge bases, making them useful for enterprise-level applications.  \n",
    "4. **Transparency and Explainability:** When using RAG, the generated answer can often be directly linked back to the source documents from which the information was retrieved. This provides a degree of explainability and allows users to verify the claims, building trust in the AI system.  \n",
    "5. **Reduced Training Costs:** Instead of constantly re-training or fine-tuning large models on new data (which is incredibly expensive and time-consuming), RAG offers a more agile way to update an LLM's knowledge by simply updating the external knowledge base.\n",
    "\n",
    "### **How RAG Works (The Process)**\n",
    "\n",
    "The RAG process typically involves three main steps:\n",
    "\n",
    "1. **Retrieval:** When a user poses a query (Q), the system first searches a vast external knowledge base (D) (e.g., a vector database containing embeddings of documents, or a conventional search index). The goal is to retrieve the most relevant passages, documents, or snippets (R) related to the query. This step often uses techniques like semantic search, keyword search (BM25), or hybrid approaches.\n",
    "\n",
    "    R=Retrieve(Q,D)  \n",
    "2. **Augmentation:** The retrieved information (R) is then concatenated or integrated with the original user query (Q) to form an augmented prompt (). This new, enriched prompt provides the LLM with the necessary context and factual grounding.\n",
    "\n",
    "    Paug​=Augment(Q,R)  \n",
    "3. **Generation:** Finally, the augmented prompt (Paug​) is fed into the LLM. The LLM then generates a coherent and accurate answer (A) based on its inherent language understanding capabilities and the specific information provided in the augmented prompt.\n",
    "\n",
    "    A=Generate(Paug​,LLM)\n",
    "\n",
    "In essence, RAG transforms an LLM from a black box of pre-trained knowledge into a dynamic, fact-checking, and context-aware reasoning engine, making it a powerful tool for building more reliable and sophisticated AI applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I understand you'd like to know which day of the week typically has the lowest employee attendance.\n",
      "\n",
      "Unfortunately, I don't have access to specific data about employee attendance across different companies or industries. The day with the lowest attendance can vary greatly depending on factors like:\n",
      "\n",
      "*   **Industry:** Some industries might have more weekend work than others.\n",
      "*   **Company Culture:** Companies with flexible work arrangements might see different patterns.\n",
      "*   **Location:** Local holidays and events can impact attendance.\n",
      "*   **Shift Schedules:** Businesses operating 24/7 might have varying attendance based on shift preferences.\n",
      "\n",
      "**How to Find This Information for Your Specific Situation:**\n",
      "\n",
      "*   **Check Your Company's Data:** The best source of information would be your company's HR or attendance records. They should be able to provide you with data on employee attendance trends.\n",
      "*   **Conduct a Survey:** If you don't have access to historical data, you could conduct an anonymous survey among your employees to gauge their preferences or reasons for absences on certain days.\n",
      "*   **Analyze Absence Reports:** Review historical absence reports to see if there are any patterns related to specific days of the week.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without any context\n",
    "\n",
    "prompt = '''\n",
    "Given the user query, answer the user to the best of your ability.\n",
    "Enusre you are polite and helpful.\n",
    "If you do not know the answer, say \"I don't know\" and offer to help with something else.\n",
    "\n",
    "User Query: {user_query}\n",
    "'''\n",
    "\n",
    "user_query = 'I need to know which day of the week is the lowest employee attendance.'\n",
    "\n",
    "response = model.generate_content(prompt.format(user_query=user_query))\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I can help you determine the day of the week with the lowest employee attendance based on the data you provided.\n",
      "\n",
      "Here's the breakdown:\n",
      "\n",
      "*   **Monday:** 0 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 0 + 1 = 4\n",
      "*   **Tuesday:** 1 + 0 + 0 + 0 + 0 + 0 + 0 + 1 + 0 + 1 = 3\n",
      "*   **Wednesday:** 0 + 1 + 0 + 1 + 0 + 0 + 1 + 0 + 0 + 1 = 4\n",
      "*   **Thursday:** 1 + 0 + 1 + 1 + 1 + 0 + 0 + 0 + 1 + 1 = 6\n",
      "*   **Friday:** 1 + 0 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1 = 5\n",
      "\n",
      "Based on these totals, **Tuesday** has the lowest employee attendance with a total of 3.\n"
     ]
    }
   ],
   "source": [
    "# With Context\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "attendance = np.random.randint(0, 2, size=(10, 5))\n",
    "\n",
    "attendance = pd.DataFrame(\n",
    "    attendance, columns=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'])\n",
    "attendance.index.name = 'Employee ID'\n",
    "\n",
    "context = attendance.to_markdown()\n",
    "\n",
    "\n",
    "prompt = '''\n",
    "Given the user query, answer the user to the best of your ability.\n",
    "Enusre you are polite and helpful.\n",
    "If you do not know the answer, say \"I don't know\" and offer to help with something else.\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "'''\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt.format(\n",
    "        user_query=user_query,\n",
    "        context=context,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Predicting Freeway Congestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supervised Learning of Query Term Relevant Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robotic Arm Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Network Intrusion Detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multi-Website Name Coreference Resolution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>Sentimental Analysis with Amazon Review Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>Bird Classification and Feature Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Human or Robot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>Predicting NBA shots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>Poverty Prediction by Selected Remote Sensing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2592 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0                         Predicting Freeway Congestion\n",
       "1     Supervised Learning of Query Term Relevant Pro...\n",
       "2                                   Robotic Arm Control\n",
       "3                           Network Intrusion Detection\n",
       "4             Multi-Website Name Coreference Resolution\n",
       "...                                                 ...\n",
       "2587       Sentimental Analysis with Amazon Review Data\n",
       "2588        Bird Classification and Feature Recognition\n",
       "2589                                     Human or Robot\n",
       "2590                               Predicting NBA shots\n",
       "2591  Poverty Prediction by Selected Remote Sensing ...\n",
       "\n",
       "[2592 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Sample Dataset\n",
    "# https://raw.githubusercontent.com/paraschopra/generating-text-small-corpus/refs/heads/master/all_ml_ideas.csv\n",
    "\n",
    "corpus = pd.read_csv(\n",
    "    'all_ml_ideas.csv',\n",
    "    sep='------',\n",
    "    engine='python',\n",
    "    header=None,\n",
    "    names=['text']\n",
    ")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Keyword Search\n",
    "\n",
    "Keyword search represents the most fundamental and intuitive form of information retrieval, forming the backbone of search engines for decades. At its core, keyword search operates on a simple premise: it matches the specific words (keywords) entered by a user in a query against the words present in a collection of documents. It's often the first interaction point for users seeking information, relying on direct lexical matches rather than deeper semantic understanding.\n",
    "\n",
    "### **The Core Principle**\n",
    "\n",
    "When a user inputs a query, a keyword search engine takes these terms and looks for their occurrences within its indexed documents. The primary goal is to identify documents that contain one or more of the specified keywords. For instance, if a user searches for \"best Italian restaurant,\" the system would attempt to find documents containing \"best,\" \"Italian,\" and \"restaurant.\"\n",
    "\n",
    "To facilitate rapid searching, keyword search engines typically rely on an **inverted index**. An inverted index is essentially a data structure that maps terms to the documents in which they appear. It looks something like this:\n",
    "\n",
    "* \"restaurant\" \\[Document 1, Document 5, Document 12\\]  \n",
    "* \"Italian\" → \\[Document 1, Document 8, Document 12\\]  \n",
    "* \"best\" → \\[Document 5, Document 12, Document 20\\]\n",
    "\n",
    "When a query comes in, the engine quickly consults this index to retrieve a list of documents containing the query terms.\n",
    "\n",
    "### **Basic Ranking and Relevance**\n",
    "\n",
    "Once a set of matching documents is identified, they need to be ranked by relevance. In its simplest form, a keyword search might rank documents based on:\n",
    "\n",
    "1. Number of Matching Keywords: Documents containing more of the query terms are considered more relevant. For example, a document containing all three terms (\"best,\" \"Italian,\" \"restaurant\") would be ranked higher than one containing only \"Italian\" and \"restaurant.\" A very simple conceptual score could be represented as:  \n",
    "   Let Q=q1​,q2​,…,qm​ be the set of keywords in the query, and D be a document.  \n",
    "2. Score(D,Q)=i=1∑m​I(qi​∈D)  \n",
    "3. where I(qi​∈D) is an indicator function that equals 1 if term qi​ is present in document D, and 0 otherwise.  \n",
    "4. **Keyword Proximity:** Documents where the keywords appear closer together are often ranked higher, as this might indicate a more coherent discussion of the topic.  \n",
    "5. **Term Position:** Keywords appearing in important fields like the document title, headings, or metadata might be given more weight than those in the body text.  \n",
    "6. **Boolean Logic:** Users can often refine queries using Boolean operators like AND (all terms must be present), OR (at least one term must be present), and NOT (a term must not be present).\n",
    "\n",
    "### **Advantages of Keyword Search**\n",
    "\n",
    "* **Simplicity:** It's easy for users to understand and for developers to implement a basic version.  \n",
    "* **Speed:** With well-optimized inverted indexes, keyword searches can be incredibly fast, even across vast document collections.  \n",
    "* **Effectiveness for Precise Queries:** When a user knows the exact terms they are looking for, keyword search can be highly effective in retrieving relevant results.\n",
    "\n",
    "### **Limitations of Keyword Search**\n",
    "\n",
    "Despite its widespread use, keyword search suffers from significant limitations:\n",
    "\n",
    "* **Lack of Semantic Understanding:** This is its most prominent drawback. Keyword search cannot grasp the underlying meaning or intent behind words. It treats \"car\" and \"automobile\" as distinct terms, failing to recognize them as synonyms. It also struggles with polysemy (words with multiple meanings, like \"bank\").  \n",
    "* **Ignores Word Order and Context:** Unless explicitly using an exact phrase search (e.g., \"machine learning\" in quotes), keyword search views text as a \"bag of words.\" It cannot distinguish between \"man bites dog\" and \"dog bites man.\"  \n",
    "* **Recall and Precision Issues:** It can suffer from low **recall** (failing to retrieve all relevant documents if they use different vocabulary) and low **precision** (retrieving irrelevant documents if keywords are too broad or common).  \n",
    "* **Spelling Sensitivity:** It is highly sensitive to typos and misspellings; a single incorrect letter can lead to zero results.\n",
    "\n",
    "In conclusion, keyword search remains a fundamental component of most search systems due to its speed and simplicity. However, its inherent limitations regarding semantic understanding and contextual awareness have driven the development of more sophisticated techniques like TF-IDF, BM25, and ultimately, neural network-based semantic search models that aim to bridge the gap between keywords and true meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the best way to learn machine learning?',\n",
       " ['best', 'way', 'learn', 'machine', 'learning'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    tokens = [\n",
    "        word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Remove special characters\n",
    "    tokens = [re.sub(r'[^a-zA-Z0-9]', '', word) for word in tokens]\n",
    "    # Remove empty strings\n",
    "    tokens = [word for word in tokens if word != '']\n",
    "    return tokens\n",
    "\n",
    "\n",
    "search_query = 'What is the best way to learn machine learning?'\n",
    "\n",
    "preprocessed_query = preprocess_text(search_query)\n",
    "\n",
    "search_query, preprocessed_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag-of-Words (BoW) model\n",
    "\n",
    "The Bag-of-Words (BoW) model is a foundational and historically significant technique in natural language processing (NLP) and information retrieval. At its core, BoW simplifies text representation by treating a document or query as an unordered collection of its words, essentially like words thrown into a \"bag.\" The key characteristic is that it disregards grammar, syntax, and even the order of words, focusing solely on the presence and frequency of individual words within the text.\n",
    "\n",
    "### **How Bag-of-Words Works for Search**\n",
    "\n",
    "For a search engine to utilize a BoW model, it first processes a large collection of documents to build a comprehensive vocabulary – a unique list of all words found across the entire corpus. Each word in this vocabulary becomes a feature or dimension in a vector space.\n",
    "\n",
    "When a document is processed, it's converted into a vector where each component corresponds to a word in the vocabulary, and its value typically represents the frequency of that word within the document. For example, if the vocabulary contains \"cat,\" \"dog,\" \"chase,\" and \"tree,\" and a document is \"The cat chases the dog,\" its BoW representation might be {\"cat\": 1, \"dog\": 1, \"chase\": 1} (ignoring common words like \"the\").\n",
    "\n",
    "When a user submits a search query, that query is also transformed into a similar BoW vector. The search engine then calculates the similarity between the query's vector and the vectors of all documents in its index. Common similarity measures include:\n",
    "\n",
    "* **Dot Product:** A simple count of shared words.  \n",
    "* **Cosine Similarity:** Measures the cosine of the angle between two vectors, indicating how similar their directions are. This is often preferred as it's normalized by document length, preventing longer documents from being unfairly favored just because they contain more words.\n",
    "\n",
    "Documents are then ranked based on their similarity scores to the query, with higher scores indicating greater relevance.\n",
    "\n",
    "### **Enhancements to the Basic BoW Model**\n",
    "\n",
    "While the core BoW is straightforward, several enhancements improve its effectiveness:\n",
    "\n",
    "* **Stop Words Removal:** Common words like \"a,\" \"the,\" \"is,\" \"and\" are often removed before creating the BoW. These words are frequent but carry little semantic meaning for distinguishing documents. Removing them reduces noise and improves efficiency.  \n",
    "* **Stemming and Lemmatization:** To account for variations of words (e.g., \"run,\" \"running,\" \"ran\"), stemming (reducing words to their root, like \"run\") or lemmatization (reducing words to their dictionary form, like \"am,\" \"is,\" \"are\" to \"be\") is applied. This ensures that different grammatical forms of the same word are treated as identical, improving matching accuracy.  \n",
    "* **TF-IDF (Term Frequency-Inverse Document Frequency):** This is a widely used weighting scheme that goes beyond simple word counts.  \n",
    "  * **Term Frequency (TF):** Measures how frequently a word appears in a document.  \n",
    "  * Inverse Document Frequency (IDF): Measures how rare a word is across the entire document collection. Words that are rare but appear in a specific document are considered more important.  \n",
    "    By multiplying TF and IDF, the BoW model gives more weight to words that are both frequent in a specific document and rare across the entire corpus, thus improving the relevance ranking.\n",
    "\n",
    "### **Advantages and Limitations**\n",
    "\n",
    "The Bag-of-Words model offers several advantages:\n",
    "\n",
    "* **Simplicity and Interpretability:** It's conceptually easy to understand and implement.  \n",
    "* **Computational Efficiency:** Building and comparing BoW representations is computationally less demanding compared to more advanced methods.  \n",
    "* **Effectiveness:** Despite its simplicity, it performs reasonably well for many fundamental text classification and retrieval tasks, serving as a strong baseline.\n",
    "\n",
    "However, BoW also has significant limitations:\n",
    "\n",
    "* **Loss of Semantic Meaning and Word Order:** This is its most critical drawback. BoW completely ignores the context, grammar, and order of words. For instance, \"The dog bit the man\" and \"The man bit the dog\" would have identical BoW representations, even though their meanings are vastly different. It cannot capture phrases or idiomatic expressions.  \n",
    "* **Synonymy and Polysemy:** It struggles with synonyms (e.g., \"car\" and \"automobile\" are treated as distinct words) and polysemy (words with multiple meanings, like \"bank\" – a river bank vs. a financial institution – are treated as the same word).  \n",
    "* **High Dimensionality and Sparsity:** For large text corpora, the vocabulary can become enormous, leading to high-dimensional vectors that are mostly empty (sparse), making computations less efficient and potentially impacting accuracy.\n",
    "\n",
    "In conclusion, while the Bag-of-Words model remains a foundational concept and is surprisingly effective for its simplicity, its inherent limitation of ignoring word order and deeper semantic relationships has paved the way for more sophisticated NLP techniques like word embeddings (Word2Vec, GloVe) and, more recently, transformer-based models (like BERT, GPT, and Gemini), which are designed to capture contextual and semantic nuances of language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'best', 'way', 'to', 'learn', 'machine', 'learning']\n",
      "|      | text                                                                                             |   score |\n",
      "|-----:|:-------------------------------------------------------------------------------------------------|--------:|\n",
      "|  807 | An application of machine learning to the board game pentago                                     |       5 |\n",
      "| 2479 | A machine learning based stock trading framework using technical and economic analysis           |       5 |\n",
      "| 2124 | Applying machine learning to the board game Pylos                                                |       5 |\n",
      "|  693 | Early defect identification of semiconductor processes using machine learning                    |       4 |\n",
      "| 2252 | Uncertainty quantification and sensitivity analysis of reservoir forecasts with machine learning |       4 |\n",
      "| 1851 | Advanced machine learning techniques for thyroid cancer diagnosis                                |       4 |\n",
      "| 1391 | CarveML an application of machine learning to file fragment classification                       |       4 |\n",
      "|  811 | Applying machine learning algorithms to oil reservoir production optimization                    |       4 |\n",
      "|  144 | Exploring machine learning techniques for recommending advertising keywords                      |       4 |\n",
      "| 2480 | Supervised learning methods for biometric authentication on mobile devices                       |       4 |\n"
     ]
    }
   ],
   "source": [
    "# Brute Force Search with Score based ranking\n",
    "\n",
    "import re\n",
    "# regex replace only to keep alphanumeric characters\n",
    "# and remove special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "search_tokens = re.sub(pattern, '', search_query).lower().split()\n",
    "\n",
    "print(search_tokens)\n",
    "\n",
    "\n",
    "def score(text, search_tokens):\n",
    "    score = 0\n",
    "    for token in search_tokens:\n",
    "        if token in text:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "\n",
    "def brute_search(corpus, search_tokens):\n",
    "    corpus = corpus.copy()\n",
    "    corpus['score'] = corpus['text'].apply(lambda x: score(x, search_tokens))\n",
    "    return corpus.sort_values(by='score', ascending=False)\n",
    "\n",
    "\n",
    "print(brute_search(corpus, search_tokens).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization and Search\n",
    "\n",
    "In the realm of information retrieval and natural language processing, simply counting words (as in the basic Bag-of-Words model) often falls short in determining the true relevance of a document to a search query. This is where TF-IDF, short for Term Frequency-Inverse Document Frequency, emerges as a powerful and widely used statistical measure. TF-IDF provides a numerical statistic that reflects how important a word is to a document in a collection or corpus.\n",
    "\n",
    "### **Breaking Down TF-IDF**\n",
    "\n",
    "The TF-IDF score for a word in a document is a product of two components:\n",
    "\n",
    "1. **Term Frequency (TF):** This measures how frequently a term (word) appears within a specific document. The idea is straightforward: if a word appears many times in a document, it's likely to be important to that document's content. However, simply using raw counts can be misleading, as longer documents will naturally have higher term frequencies for most words. To mitigate this, TF is often normalized, for example, by dividing the raw count by the total number of terms in the document. TF(t,d)=Total number of terms in document dNumber of times term t appears in document d​  \n",
    "2. **Inverse Document Frequency (IDF):** While TF highlights words frequent within a document, it doesn't account for the overall importance of the word across the entire collection of documents. Words like \"the,\" \"a,\" and \"is\" appear frequently in almost all documents but carry little unique meaning or discriminative power. IDF addresses this by assigning a higher weight to terms that are rare across the entire corpus and a lower weight to common terms. IDF(t,D)=log(Number of documents d containing term tTotal number of documents N​) The logarithm helps to scale the values. A higher IDF indicates that the term is rare and therefore more significant.\n",
    "\n",
    "### **The TF-IDF Score and Vectorization**\n",
    "\n",
    "The TF-IDF score for a term in a document is then calculated by multiplying its TF and IDF values: TFIDF(t,d,D)=TF(t,d)×IDF(t,D)\n",
    "\n",
    "This combined score assigns a high value to terms that appear frequently in a particular document but rarely in the overall corpus. Conversely, terms that are common across many documents (like stop words) or very rare in a single document will have low TF-IDF scores.\n",
    "\n",
    "**TF-IDF Vectorization** involves representing each document in a collection as a vector in a high-dimensional space. Each dimension of this vector corresponds to a unique term in the vocabulary of the entire corpus. The value for each dimension in a document's vector is the TF-IDF score of the corresponding term within that document. This process transforms unstructured text into a structured numerical format that machine learning algorithms can easily process.\n",
    "\n",
    "### **TF-IDF for Search**\n",
    "\n",
    "When a user submits a search query, the same TF-IDF vectorization process is applied to the query itself. The query is treated as a short document, and its terms are assigned TF-IDF weights based on their frequency in the query and their inverse document frequency across the entire document collection.\n",
    "\n",
    "Once both the query and all documents in the collection are represented as TF-IDF vectors, search becomes a problem of **vector space model similarity**. The search engine calculates the similarity between the query vector and each document vector. The most common similarity metric used for this is **Cosine Similarity**, which measures the cosine of the angle between two vectors. A cosine similarity closer to 1 indicates higher similarity (vectors pointing in the same direction), meaning the document is highly relevant to the query.\n",
    "\n",
    "The documents are then ranked in descending order of their cosine similarity scores, presenting the most relevant results to the user first.\n",
    "\n",
    "### **Advantages and Limitations**\n",
    "\n",
    "**Advantages of TF-IDF:**\n",
    "\n",
    "* **Improved Relevance:** By giving more weight to rare and important terms, TF-IDF often produces more relevant search results compared to simple Bag-of-Words counts.  \n",
    "* **Handles Common Words:** The IDF component effectively down-weights common, less informative words, preventing them from dominating the similarity scores.  \n",
    "* **Simplicity and Scalability:** It's relatively simple to compute and scales well to large document collections.\n",
    "\n",
    "**Limitations of TF-IDF:**\n",
    "\n",
    "* **Semantic Blindness:** Like the basic BoW model, TF-IDF still operates at the word level and completely ignores the semantic relationships between words (e.g., \"car\" and \"automobile\" are treated as distinct terms). It cannot understand context or nuances of language.  \n",
    "* **Word Order Ignorance:** It does not consider the order of words, meaning phrases and the grammatical structure of sentences are lost.  \n",
    "* **Sparse Vectors:** For very large vocabularies, TF-IDF vectors can be extremely high-dimensional and sparse (mostly zeros), which can be computationally inefficient for some operations.\n",
    "\n",
    "Despite its limitations, TF-IDF remains a cornerstone in information retrieval, often used as a baseline or as a feature engineering technique in more complex systems. Its effectiveness and simplicity make it a valuable tool for understanding and processing text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      | text                                                               |   similarity |\n",
      "|-----:|:-------------------------------------------------------------------|-------------:|\n",
      "|  699 | Finding the Best Photo                                             |     0.358477 |\n",
      "| 1550 | What Can You Learn From Accelerometer Data                         |     0.319214 |\n",
      "|  675 | Can a machine learn to teach?                                      |     0.30413  |\n",
      "| 1764 | What can we learn from a movie's color?                            |     0.298678 |\n",
      "| 1983 | Finding Your Way, Courtesy of Machine Learning                     |     0.285296 |\n",
      "| 1520 | A novel way to Soccer Match Prediction                             |     0.260768 |\n",
      "| 1991 | Learn To Rate Fine Food                                            |     0.233312 |\n",
      "| 1882 | The Price is Right? Estimating Medical Costs with Machine Learning |     0.2305   |\n",
      "|   26 | This Sentence is Not a Question, or Is It?                         |     0.22556  |\n",
      "|  522 | 4-Way-Stop Wait-Time Prediction                                    |     0.214432 |\n"
     ]
    }
   ],
   "source": [
    "# Building a TF-IDF Vectorizer based search engine\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus['text'])\n",
    "\n",
    "\n",
    "def search_tfidf(corpus, search_query):\n",
    "    corpus = corpus.copy()\n",
    "    search_query_vector = vectorizer.transform([search_query])\n",
    "    similarity = cosine_similarity(search_query_vector, tfidf_matrix).flatten()\n",
    "    corpus['similarity'] = similarity\n",
    "    return corpus.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "\n",
    "print(search_tfidf(corpus, search_query).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While TF-IDF (Term Frequency-Inverse Document Frequency) has been a cornerstone of information retrieval for decades, the Okapi BM25 (Best Match 25\\) ranking function emerged as a more sophisticated and often more effective approach to calculating document relevance for a given search query. BM25 is a probabilistic retrieval model that builds upon the TF-IDF concept, but with significant improvements, particularly in how it handles term frequency saturation and document length normalization. It's widely used in modern search engines due to its empirical effectiveness.\n",
    "\n",
    "### **How BM25 Works**\n",
    "\n",
    "BM25 calculates a relevance score for each document with respect to a search query. This score is a sum of the scores for each term in the query. For a query containing terms q1​,…,qn​, the BM25 score for a document D is typically calculated as:\n",
    "\n",
    "$$\n",
    "Score(D, Q) = \\sum_{i=1}^{n} IDF(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|D|}{avgdl}\\right)}\n",
    "$$\n",
    "Let's break down the components:\n",
    "\n",
    "1. **IDF (Inverse Document Frequency):** Similar to TF-IDF, BM25 uses IDF to weigh terms. Rare terms across the corpus contribute more to the total score than common terms. The formula for IDF in BM25 often uses a slightly different variant to the standard log function in TF-IDF, sometimes including smoothing terms to avoid division by zero for terms appearing in no documents. IDF(qi​) is the Inverse Document Frequency of term qi​.  \n",
    "2. **Term Frequency (**f(qi​,D)**):** This represents how many times term qi​ appears in document D. Unlike raw TF, BM25 incorporates parameters k1​ and b to prevent term frequency from saturating.  \n",
    "   * **Saturation:** In simple TF, a word appearing 100 times in a document contributes 100 times as much as a word appearing once. BM25 acknowledges that beyond a certain point, additional occurrences of a term provide diminishing returns to relevance. The k1​ parameter controls this saturation point. Higher values of k1​ lead to less saturation.  \n",
    "   * The term f(qi​,D)+k1​f(qi​,D)⋅(k1​+1)​ is the core of how BM25 handles term frequency, allowing it to give more weight to more frequent terms up to a point, then the contribution levels off.  \n",
    "3. **Document Length Normalization (**∣D∣ **and** avgdl**):** This part of the formula addresses the issue of longer documents potentially having higher raw term frequencies simply because they are longer.  \n",
    "   * ∣D∣ is the length of the document D (e.g., in words).  \n",
    "   * avgdl is the average document length in the corpus.  \n",
    "   * The parameter b (often between 0 and 1\\) controls the degree of document length normalization.  \n",
    "     * If b=0, no length normalization is applied.  \n",
    "     * If b=1, full length normalization is applied. The term (1−b+b⋅avgdl∣D∣​) scales the denominator, effectively reducing the impact of term frequency in longer documents relative to shorter ones, especially if the term is not particularly frequent in the longer document compared to its overall length.\n",
    "\n",
    "### **BM25 for Search**\n",
    "\n",
    "When a query is submitted, each document's BM25 score is calculated based on the query terms. Documents are then ranked by their BM25 scores, from highest to lowest, to present the most relevant results. The parameters k1​ (typically between 1.2 and 2.0) and b (typically around 0.75) are tunable and often optimized based on the specific characteristics of the document corpus and search task.\n",
    "\n",
    "### **Advantages and Limitations**\n",
    "\n",
    "**Advantages of BM25:**\n",
    "\n",
    "* **Improved Relevance:** BM25 generally outperforms plain TF-IDF in most retrieval scenarios due to its refined handling of term frequency saturation and document length normalization.  \n",
    "* **Probabilistic Foundation:** Its roots in probabilistic models provide a more theoretically sound basis for relevance estimation compared to the heuristic nature of raw TF-IDF.  \n",
    "* **Robustness:** It is relatively robust and performs well across diverse datasets without extensive tuning.\n",
    "\n",
    "**Limitations of BM25:**\n",
    "\n",
    "* **Semantic Gap:** Like TF-IDF, BM25 is a keyword-matching algorithm. It still suffers from the \"semantic gap\" problem, meaning it doesn't understand synonyms (e.g., \"car\" and \"automobile\" are treated as distinct) or the deeper meaning and context of words.  \n",
    "* **Word Order Ignorance:** It treats the query and documents as a \"bag of words,\" ignoring the order of words and thus failing to capture phrases or idiomatic expressions.  \n",
    "* **Parameter Tuning:** While robust, optimal performance can sometimes require careful tuning of k1​ and b for specific corpora.  \n",
    "* **Outperformed by Modern Methods:** For complex search tasks requiring deep semantic understanding, BM25 is often surpassed by neural network-based models and dense retrieval methods (like those using word embeddings or transformer models) that capture contextual relationships.\n",
    "\n",
    "Despite the emergence of more advanced neural retrieval methods, BM25 remains a highly effective, computationally efficient, and widely used ranking function in many practical search systems, often serving as a strong baseline or even a crucial component in hybrid retrieval architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr/local/google/home/vermavineet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usr/local/google/home/vermavineet/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      | text                                           | tokens                                                |    score |\n",
      "|-----:|:-----------------------------------------------|:------------------------------------------------------|---------:|\n",
      "| 1983 | Finding Your Way, Courtesy of Machine Learning | ['finding', 'way', 'courtesy', 'machine', 'learning'] | 10.7682  |\n",
      "|  675 | Can a machine learn to teach?                  | ['machine', 'learn', 'teach']                         | 10.0588  |\n",
      "|  699 | Finding the Best Photo                         | ['finding', 'best', 'photo']                          |  9.37019 |\n",
      "| 1708 | Win Some, Learn Some                           | ['win', 'learn']                                      |  8.01062 |\n",
      "| 1359 | Can Machines Learn Genres                      | ['machines', 'learn', 'genres']                       |  7.18643 |\n",
      "|  885 | How well do people learn?                      | ['well', 'people', 'learn']                           |  7.18643 |\n",
      "| 1550 | What Can You Learn From Accelerometer Data     | ['learn', 'accelerometer', 'data']                    |  7.18643 |\n",
      "| 1520 | A novel way to Soccer Match Prediction         | ['novel', 'way', 'soccer', 'match', 'prediction']     |  6.88701 |\n",
      "| 1991 | Learn To Rate Fine Food                        | ['learn', 'rate', 'fine', 'food']                     |  6.51602 |\n",
      "| 1764 | What can we learn from a movie's color?        | ['learn', 'movie', 's', 'color']                      |  6.51602 |\n"
     ]
    }
   ],
   "source": [
    "# rank_bm25 based search engine\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    tokens = [\n",
    "        word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Remove special characters\n",
    "    tokens = [re.sub(r'[^a-zA-Z0-9]', '', word) for word in tokens]\n",
    "    # Remove empty strings\n",
    "    tokens = [word for word in tokens if word != '']\n",
    "    return tokens\n",
    "\n",
    "\n",
    "corpus['tokens'] = corpus['text'].apply(preprocess_text)\n",
    "corpus['tokens'].head(10).to_markdown()\n",
    "# BM25\n",
    "bm25 = BM25Okapi(corpus['tokens'].tolist())\n",
    "\n",
    "\n",
    "def search_bm25(corpus, search_query):\n",
    "    corpus = corpus.copy()\n",
    "    search_query_tokens = preprocess_text(search_query)\n",
    "    scores = bm25.get_scores(search_query_tokens)\n",
    "    corpus['score'] = scores\n",
    "    return corpus.sort_values(by='score', ascending=False)\n",
    "\n",
    "\n",
    "print(search_bm25(corpus, search_query).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embeddings on text\n",
    "\n",
    "In the realm of natural language processing (NLP), computers don't inherently understand human language. Words, phrases, and documents are abstract concepts to a machine. **Vector embeddings** are a revolutionary technique that bridges this gap by converting textual data into dense numerical representations, or vectors, in a continuous vector space. These vectors capture the semantic meaning and contextual relationships of words and phrases, allowing machines to process and \"understand\" language in a way that was previously impossible with traditional methods like Bag-of-Words or TF-IDF.\n",
    "\n",
    "### **What are Vector Embeddings?**\n",
    "\n",
    "At its core, a vector embedding is an array of real numbers (e.g., `[0.1, -0.5, 0.8, 0.2, ...]`) that represents a piece of text. Each number in the array corresponds to a dimension in the vector space. The magic lies in how these numbers are assigned: words or phrases that are semantically similar tend to be located closer together in this high-dimensional space, while dissimilar terms are further apart. For instance, the vector for \"king\" might be close to \"queen\" and \"man,\" and the vector difference between \"king\" and \"man\" could be similar to the difference between \"queen\" and \"woman.\"\n",
    "\n",
    "### **Why are They Needed?**\n",
    "\n",
    "Traditional text representation methods like Bag-of-Words or TF-IDF suffer from significant limitations:\n",
    "\n",
    "* **Lack of Semantic Understanding:** They treat words as independent entities, ignoring synonyms, polysemy, and the underlying meaning. \"Car\" and \"automobile\" would be seen as entirely different terms.  \n",
    "* **High Dimensionality and Sparsity:** For large vocabularies, vectors can become extremely long and mostly filled with zeros, leading to inefficient computation and storage.  \n",
    "* **No Contextual Information:** They cannot capture the meaning of a word based on its surrounding words.\n",
    "\n",
    "Vector embeddings overcome these limitations by:\n",
    "\n",
    "* **Capturing Semantic Similarity:** They embed words with similar meanings close to each other in the vector space.  \n",
    "* **Capturing Relationships:** They can learn analogies and relationships between words.  \n",
    "* **Reduced Dimensionality:** They represent text in dense, lower-dimensional vectors compared to sparse, high-dimensional representations.\n",
    "\n",
    "### **How are Vector Embeddings Generated?**\n",
    "\n",
    "Vector embeddings are typically learned from vast amounts of text data using various machine learning techniques:\n",
    "\n",
    "1. **Context-Independent Embeddings (e.g., Word2Vec, GloVe):** These models learn a single, fixed embedding for each word, regardless of its context. They analyze the co-occurrence patterns of words in a large corpus. If words frequently appear together or in similar contexts, their embeddings will be closer.  \n",
    "2. **Contextual Embeddings (e.g., BERT, GPT, Gemini Embeddings):** More advanced models, particularly those based on the Transformer architecture, generate embeddings that are *context-dependent*. This means the embedding for a word like \"bank\" would be different if it's used in the context of a \"river bank\" versus a \"financial bank.\" These models process entire sentences or paragraphs to generate rich, context-aware representations for each word or even for the entire input.\n",
    "\n",
    "### **How are Vector Embeddings Used in Search?**\n",
    "\n",
    "Vector embeddings are crucial for **semantic search**, which goes beyond simple keyword matching:\n",
    "\n",
    "1. **Vectorizing Documents:** All documents in a corpus are transformed into their corresponding embedding vectors using a pre-trained embedding model. These vectors are then stored in a specialized database called a **vector database** (or vector index).  \n",
    "2. **Vectorizing Queries:** When a user submits a query, it is also converted into an embedding vector using the *same* embedding model.  \n",
    "3. **Similarity Search:** Instead of matching keywords, the search engine now performs a **similarity search** in the vector space. It finds document vectors that are \"closest\" to the query vector. The most common metric for measuring this similarity is **Cosine Similarity**: Let and B be two embedding vectors.\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "\n",
    "A cosine similarity score close to 1 indicates high semantic similarity.\n",
    "\n",
    "Documents are then ranked based on their cosine similarity scores to the query vector. This allows the search engine to retrieve documents that might not contain the exact keywords but are semantically related to the query, significantly improving search relevance and recall.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Vector embeddings have transformed NLP, enabling machines to grasp the nuances of human language. They are a cornerstone of modern search, recommendation systems, clustering, and many other AI applications, pushing the boundaries of what's possible in understanding and interacting with text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The King won ...</th>\n",
       "      <th>The Man won ...</th>\n",
       "      <th>The Queen won ...</th>\n",
       "      <th>The Woman won ...</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The King won ...</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Man won ...</th>\n",
       "      <td>0.830</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Queen won ...</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.748</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Woman won ...</th>\n",
       "      <td>0.712</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.803</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   The King won ...  The Man won ...  The Queen won ...  \\\n",
       "The King won ...              1.000            0.830              0.866   \n",
       "The Man won ...               0.830            1.000              0.748   \n",
       "The Queen won ...             0.866            0.748              1.000   \n",
       "The Woman won ...             0.712            0.751              0.803   \n",
       "\n",
       "                   The Woman won ...  \n",
       "The King won ...               0.712  \n",
       "The Man won ...                0.751  \n",
       "The Queen won ...              0.803  \n",
       "The Woman won ...              1.000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from typing import List\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "HF_EMBEDDING_MODEL = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")\n",
    "\n",
    "\n",
    "def generate_embeddings(contents: List[str], embedding_model=HF_EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the given content using the Google Gemini model.\n",
    "    \"\"\"\n",
    "    response = embedding_model.embed_documents(contents)\n",
    "    return np.array(response)\n",
    "\n",
    "\n",
    "statements = [\n",
    "    'The King won the battle but lost the war.',\n",
    "    'The Man won the battle but lost the war.',\n",
    "    'The Queen won the battle but lost the war.',\n",
    "    'The Woman won the battle but lost the war.',\n",
    "]\n",
    "\n",
    "all_embeddings = generate_embeddings(statements)\n",
    "\n",
    "similarity = cosine_similarity(all_embeddings, all_embeddings)\n",
    "\n",
    "_statements = [statement.replace(\n",
    "    'the battle but lost the war.', '...') for statement in statements]\n",
    "\n",
    "df = pd.DataFrame(similarity, columns=_statements)\n",
    "\n",
    "df.index = _statements\n",
    "\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for similar vectors using Cosine Similarity\n",
    "\n",
    "In the realm of modern information retrieval and recommendation systems, **vector embeddings** allow us to represent items (like text documents, images, or user preferences) as numerical vectors in a high-dimensional space. Once text is converted into these meaningful vector representations, the task of finding \"similar\" items transforms into a geometric problem: identifying vectors that are \"close\" to each other in this space. Among various distance or similarity metrics, **Cosine Similarity** stands out as a popular and effective choice for this purpose.\n",
    "\n",
    "### **The Role of Cosine Similarity**\n",
    "\n",
    "Cosine Similarity measures the cosine of the angle between two non-zero vectors. It determines whether two vectors are pointing in roughly the same direction, which indicates semantic similarity, regardless of their magnitude (length). A cosine similarity score ranges from \\-1 to 1:\n",
    "\n",
    "* **1:** Indicates identical direction (maximum similarity).  \n",
    "* **0:** Indicates orthogonality (no correlation).  \n",
    "* **\\-1:** Indicates diametrically opposite direction (maximum dissimilarity).\n",
    "\n",
    "The formula for cosine similarity between two vectors A and B is:\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\n",
    "$$\n",
    "\n",
    "Here, A⋅B is the dot product of the vectors, and ∣∣A∣∣ and ∣∣B∣∣ are their respective magnitudes (Euclidean norms).\n",
    "\n",
    "### **Searching for Similar Vectors**\n",
    "\n",
    "The process of searching for similar vectors using cosine similarity typically involves:\n",
    "\n",
    "1. **Embedding Generation:** A query (e.g., a search phrase) is first converted into its corresponding vector embedding using the same model used for embedding the documents or items in your database.  \n",
    "2. **Similarity Calculation:** This query vector is then compared against the pre-computed embedding vectors of all documents or items in your collection. For each comparison, a cosine similarity score is calculated.  \n",
    "3. **Ranking:** The documents/items are then ranked in descending order based on their cosine similarity scores, with those having the highest positive scores considered most similar and thus most relevant to the query.\n",
    "\n",
    "This approach allows for semantic search, where results are based on meaning rather than just keyword matching, making it powerful for tasks like question answering, content recommendation, and plagiarism detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.722, 'The Man won the battle but lost the war.'),\n",
       " (0.618, 'The King won the battle but lost the war.'),\n",
       " (0.583, 'The Woman won the battle but lost the war.'),\n",
       " (0.568, 'The Queen won the battle but lost the war.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = 'The Soldier lost all wars'\n",
    "search_embedding = generate_embeddings([sq])\n",
    "\n",
    "similarity = cosine_similarity(\n",
    "    search_embedding.reshape(1, -1), all_embeddings\n",
    ")\n",
    "\n",
    "scores = [(score, statement)\n",
    "          for score, statement in zip(similarity[0].round(3), statements)]\n",
    "\n",
    "best_matches = sorted(scores, reverse=True, key=lambda x: x[0])\n",
    "\n",
    "best_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>Finding Your Way, Courtesy of Machine Learning</td>\n",
       "      <td>0.583221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>Machine Learning In JavaScript</td>\n",
       "      <td>0.561169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>How can machine learning help stock investment?</td>\n",
       "      <td>0.536195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>Parallelizing Machine Learning Algorithms</td>\n",
       "      <td>0.524660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>Exploring the Efficacy of Machine Learning in ...</td>\n",
       "      <td>0.519362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Can a machine learn to teach?</td>\n",
       "      <td>0.519019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>Can Machines Learn Genres</td>\n",
       "      <td>0.517664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>Machine Learning in Stock Price Trend Forecasting</td>\n",
       "      <td>0.505323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>An Empirical Study of Machine Learning Techniq...</td>\n",
       "      <td>0.504424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>Implementing Machine Learning to Earthquake En...</td>\n",
       "      <td>0.502205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  similarity\n",
       "1983     Finding Your Way, Courtesy of Machine Learning    0.583221\n",
       "1449                     Machine Learning In JavaScript    0.561169\n",
       "1780    How can machine learning help stock investment?    0.536195\n",
       "606           Parallelizing Machine Learning Algorithms    0.524660\n",
       "1981  Exploring the Efficacy of Machine Learning in ...    0.519362\n",
       "675                       Can a machine learn to teach?    0.519019\n",
       "1359                          Can Machines Learn Genres    0.517664\n",
       "1225  Machine Learning in Stock Price Trend Forecasting    0.505323\n",
       "413   An Empirical Study of Machine Learning Techniq...    0.504424\n",
       "2240  Implementing Machine Learning to Earthquake En...    0.502205"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets load a larger corpus of text\n",
    "\n",
    "# Vector Based Search Engine\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Generating embeddings\")\n",
    "\n",
    "\n",
    "def search(\n",
    "        query: str,\n",
    "        vector_database: pd.DataFrame,\n",
    "        number_of_results: int = 10,\n",
    "        embedding_model=HF_EMBEDDING_MODEL\n",
    "):\n",
    "    \"\"\"\n",
    "    Search for the most similar content in the vector database.\n",
    "    \"\"\"\n",
    "    search_embedding = generate_embeddings(\n",
    "        [query],\n",
    "        embedding_model=embedding_model\n",
    "    )[0]\n",
    "    results = vector_database.apply(\n",
    "        lambda x: cosine_similarity(\n",
    "            x.embedding.reshape(1, -1), search_embedding.reshape(1, -1)\n",
    "        )[0][0], axis=1\n",
    "    ).sort_values(ascending=False).head(number_of_results)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            'text': vector_database.iloc[results.index].text,\n",
    "            'similarity': results.values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Build In Memory Vector Database\n",
    "vector_database = pd.DataFrame({'text': corpus.text, 'embedding': [\n",
    "    i for i in generate_embeddings(corpus.text)]})\n",
    "\n",
    "# Vector Based Search\n",
    "\n",
    "results = search(query=search_query, vector_database=vector_database)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
